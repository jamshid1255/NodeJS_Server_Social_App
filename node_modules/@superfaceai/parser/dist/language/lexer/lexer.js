"use strict";
var _a;
Object.defineProperty(exports, "__esModule", { value: true });
exports.Lexer = exports.DEFAULT_TOKEN_KIND_FILTER = void 0;
var source_1 = require("../../common/source");
var error_1 = require("../error");
var default_1 = require("./sublexer/default");
var jessie_1 = require("./sublexer/jessie");
var token_1 = require("./token");
exports.DEFAULT_TOKEN_KIND_FILTER = (_a = {},
    _a[6 /* COMMENT */] = true,
    _a[7 /* NEWLINE */] = true,
    _a[5 /* IDENTIFIER */] = false,
    _a[3 /* LITERAL */] = false,
    _a[2 /* OPERATOR */] = false,
    _a[1 /* SEPARATOR */] = false,
    _a[4 /* STRING */] = false,
    _a[8 /* JESSIE_SCRIPT */] = false,
    _a[0 /* UNKNOWN */] = false,
    _a);
/**
 * Lexer tokenizes input string into tokens.
 *
 * The lexer generates a stream of tokens, always starting with SEPARATOR SOF and always ending with SEPARATOR EOF.
 * The stream can be consumed by calling `advance`. After each advance, `lookahead` will provide access to the next
 * token without consuming it.
 * After EOF is emitted, all further calls to `advance` and `lookahead` will return the same EOF.
 *
 * An optional `tokenKindFilter` parameter can be provided to filter
 * the tokens returned by `advance` and `lookahead`. By default, this filter skips comment nodes.
 *
 * The advance function also accepts an optional `context` parameter which can be used to control the lexer context
 * for the next token.
 */
var Lexer = /** @class */ (function () {
    function Lexer(source, tokenKindFilter) {
        var _a;
        this.source = source;
        /** Stores whether the SOF and EOF were yielded. */
        this.fileSeparatorYielded = false;
        this.sublexers = (_a = {},
            _a[0 /* DEFAULT */] = default_1.tryParseDefault,
            _a[1 /* JESSIE_SCRIPT_EXPRESSION */] = jessie_1.tryParseJessieScriptExpression,
            _a);
        this.currentToken = new token_1.LexerToken({
            kind: 1 /* SEPARATOR */,
            separator: 'SOF',
        }, {
            start: {
                line: 1,
                column: 1,
                charIndex: 0,
            },
            end: {
                line: 1,
                column: 1,
                charIndex: 0,
            },
        });
        this.tokenKindFilter = tokenKindFilter !== null && tokenKindFilter !== void 0 ? tokenKindFilter : exports.DEFAULT_TOKEN_KIND_FILTER;
    }
    /** Advances the lexer returning the current token. */
    Lexer.prototype.advance = function (context) {
        if (this.currentToken.isEOF()) {
            this.fileSeparatorYielded = true;
            return this.currentToken;
        }
        if (this.currentToken.isSOF() && !this.fileSeparatorYielded) {
            this.fileSeparatorYielded = true;
            return this.currentToken;
        }
        this.currentToken = this.lookahead(context);
        this.fileSeparatorYielded = false;
        return this.currentToken;
    };
    /** Returns the next token without advancing the lexer. */
    Lexer.prototype.lookahead = function (context) {
        // EOF forever
        if (this.currentToken.isEOF()) {
            return this.currentToken;
        }
        // read next token
        var nextToken = this.readNextToken(this.currentToken, context);
        // skip tokens if they are caught by the filter
        while (this.tokenKindFilter[nextToken.data.kind]) {
            // Always break on EOF even if separators are filtered to avoid an infinite loop.
            if (nextToken.isEOF()) {
                break;
            }
            nextToken = this.readNextToken(nextToken, context);
        }
        return nextToken;
    };
    Lexer.prototype.next = function (context) {
        var tok = this.advance(context);
        // Ensure that EOF is yielded once
        if (tok.isEOF() && this.fileSeparatorYielded) {
            return {
                done: true,
                value: undefined,
            };
        }
        return {
            done: false,
            value: tok,
        };
    };
    Lexer.prototype.return = function (value) {
        return {
            done: true,
            value: value,
        };
    };
    Lexer.prototype.throw = function (e) {
        throw e;
    };
    Lexer.prototype[Symbol.iterator] = function () {
        return this;
    };
    Lexer.prototype.peek = function (context) {
        var tok = this.lookahead(context);
        if (tok.isEOF() && this.currentToken.isEOF()) {
            return {
                done: true,
                value: undefined,
            };
        }
        return {
            done: false,
            value: tok,
        };
    };
    /** Saves the lexer state to be restored later. */
    Lexer.prototype.save = function () {
        return [this.currentToken, this.fileSeparatorYielded];
    };
    /**
     * Roll back the state of the lexer to the given saved state.
     *
     * The lexer will continue from this state forward.
     */
    Lexer.prototype.rollback = function (state) {
        this.currentToken = state[0];
        this.fileSeparatorYielded = state[1];
    };
    /**
     * Compute start location of the token following `lastToken`.
     */
    Lexer.prototype.computeNextTokenStartLocation = function (lastToken) {
        // Count number of non-newline whitespace tokens after the last token.
        var whitespaceAfterLast = (0, source_1.countStarting)(function (ch) { return !(0, source_1.isNewline)(ch) && (0, source_1.isWhitespace)(ch); }, this.source.body.slice(lastToken.location.end.charIndex));
        // Since we already know the end location of the last token and we only cound non-newline whitespace
        // we can obtain the new location trivially.
        return {
            line: lastToken.location.end.line,
            column: lastToken.location.end.column + whitespaceAfterLast,
            charIndex: lastToken.location.end.charIndex + whitespaceAfterLast,
        };
    };
    /** Reads the next token following the `afterPosition`. */
    Lexer.prototype.readNextToken = function (lastToken, context) {
        var startLocation = this.computeNextTokenStartLocation(lastToken);
        var slice = this.source.body.slice(startLocation.charIndex);
        // Call one of the sublexers
        var tokenParseResult;
        if (context === undefined) {
            context = { type: 0 /* DEFAULT */ };
        }
        switch (context.type) {
            case 0 /* DEFAULT */:
                tokenParseResult = this.sublexers[0 /* DEFAULT */](slice);
                break;
            case 1 /* JESSIE_SCRIPT_EXPRESSION */:
                tokenParseResult = this.sublexers[1 /* JESSIE_SCRIPT_EXPRESSION */](slice, context.terminationTokens);
                break;
        }
        // For errors, there are two spans and two locations here:
        // * the location of the token which produced this error during lexing
        // * the location of the error within that token
        //
        // the entire span of the token is unknown, as the error has happened along the way
        // but we know it covers the error span at least
        // Didn't parse as any known token or produced an error
        if (tokenParseResult.kind === 'nomatch') {
            // here we couldn't match anything, so the token span and the error span are the same
            // we assume the token wasn't a newline (since we can parse that token)
            var tokenLocationSpan = {
                start: startLocation,
                end: {
                    line: startLocation.line,
                    column: startLocation.column + 1,
                    charIndex: startLocation.charIndex + 1,
                },
            };
            var error = new error_1.SyntaxError(this.source, tokenLocationSpan, "Lexer" /* LEXER */, 'Could not match any token');
            return new token_1.LexerToken({
                kind: 0 /* UNKNOWN */,
                error: error,
            }, tokenLocationSpan);
        }
        // Produced an error
        if (tokenParseResult.kind === 'error') {
            var category = void 0;
            var detail = void 0;
            var hints = void 0;
            var relativeSpan = void 0;
            // Single-error results are easy
            if (tokenParseResult.errors.length === 1) {
                var error_2 = tokenParseResult.errors[0];
                category = error_2.category;
                detail = error_2.detail;
                hints = error_2.hints;
                relativeSpan = error_2.relativeSpan;
            }
            else {
                // multi-error results combine all errors and hints into one, the span is the one that covers all the errors
                category = tokenParseResult.errors
                    .map(function (e) { return e.category; })
                    .reduce(function (acc, curr) {
                    if (acc === curr) {
                        return acc;
                    }
                    else {
                        return "Lexer" /* LEXER */;
                    }
                });
                detail = tokenParseResult.errors
                    .map(function (err) { var _a; return (_a = err.detail) !== null && _a !== void 0 ? _a : ''; })
                    .join('; ');
                hints = tokenParseResult.errors.flatMap(function (err) { return err.hints; });
                relativeSpan = tokenParseResult.errors
                    .map(function (err) { return err.relativeSpan; })
                    .reduce(function (acc, curr) {
                    return {
                        start: Math.min(acc.start, curr.start),
                        end: Math.max(acc.end, curr.end),
                    };
                });
            }
            // here the error span and the token span (and location) are different
            var tokenLocation = {
                start: startLocation,
                end: (0, source_1.computeEndLocation)(this.source.body.slice(startLocation.charIndex, startLocation.charIndex + relativeSpan.end), startLocation),
            };
            var errorLocation = {
                start: (0, source_1.computeEndLocation)(this.source.body.slice(tokenLocation.start.charIndex, tokenLocation.start.charIndex + relativeSpan.start), tokenLocation.start),
                end: (0, source_1.computeEndLocation)(this.source.body.slice(tokenLocation.start.charIndex, tokenLocation.start.charIndex + relativeSpan.end), tokenLocation.start),
            };
            var error = new error_1.SyntaxError(this.source, errorLocation, category, detail, hints);
            return new token_1.LexerToken({
                kind: 0 /* UNKNOWN */,
                error: error,
            }, tokenLocation);
        }
        var parsedTokenLocation = {
            start: (0, source_1.computeEndLocation)(this.source.body.slice(startLocation.charIndex, startLocation.charIndex + tokenParseResult.relativeSpan.start), startLocation),
            end: (0, source_1.computeEndLocation)(this.source.body.slice(startLocation.charIndex, startLocation.charIndex + tokenParseResult.relativeSpan.end), startLocation),
        };
        // All is well
        return new token_1.LexerToken(tokenParseResult.data, parsedTokenLocation);
    };
    return Lexer;
}());
exports.Lexer = Lexer;
//# sourceMappingURL=lexer.js.map